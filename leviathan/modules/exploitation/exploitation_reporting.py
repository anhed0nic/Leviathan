"""Exploitation reporting module for Leviathan."""

import json
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
from pathlib import Path
import statistics

from ...core.module_base import AnalysisModule
from ...utils.logging import get_logger


class ExploitationReporting(AnalysisModule):
    """Generates comprehensive reports on exploitation activities and safety metrics."""

    def __init__(self, config=None):
        super().__init__(config)
        self.logger = get_logger("leviathan.exploitation.reporting")

        # Reporting state
        self.reports_history = []
        self.metrics_aggregator = {}

    @property
    def name(self) -> str:
        return "exploitation_reporting"

    @property
    def description(self) -> str:
        return "Generates comprehensive reports on exploitation activities and safety metrics"

    async def analyze(self, target: Any) -> Dict[str, Any]:
        """Generate exploitation report based on provided data."""
        if not isinstance(target, dict):
            raise ValueError("Target must be a report configuration dictionary")

        report_config = target.get("report_config", {})
        exploitation_data = target.get("exploitation_data", {})
        safety_metrics = target.get("safety_metrics", {})
        performance_data = target.get("performance_data", {})

        report_type = report_config.get("type", "comprehensive")
        report_id = f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        self.logger.info(
            "Generating exploitation report",
            report_id=report_id,
            type=report_type
        )

        try:
            if report_type == "comprehensive":
                report = await self._generate_comprehensive_report(
                    report_id, exploitation_data, safety_metrics, performance_data, report_config
                )
            elif report_type == "safety":
                report = await self._generate_safety_report(
                    report_id, safety_metrics, report_config
                )
            elif report_type == "performance":
                report = await self._generate_performance_report(
                    report_id, performance_data, report_config
                )
            elif report_type == "vulnerability":
                report = await self._generate_vulnerability_report(
                    report_id, exploitation_data, report_config
                )
            else:
                raise ValueError(f"Unknown report type: {report_type}")

            # Save report to history
            self.reports_history.append(report)

            # Export report if requested
            if report_config.get("export_path"):
                await self._export_report(report, report_config["export_path"])

            return {
                "module": self.name,
                "report_id": report_id,
                "report_type": report_type,
                "success": True,
                "sections": list(report.keys()),
                "generated_at": report.get("generated_at"),
                "exported": bool(report_config.get("export_path"))
            }

        except Exception as e:
            self.logger.error("Report generation failed", report_id=report_id, error=str(e))
            return {
                "module": self.name,
                "report_id": report_id,
                "success": False,
                "error": str(e)
            }

    async def _generate_comprehensive_report(self, report_id: str, exploitation_data: Dict[str, Any],
                                           safety_metrics: Dict[str, Any], performance_data: Dict[str, Any],
                                           config: Dict[str, Any]) -> Dict[str, Any]:
        """Generate a comprehensive exploitation report."""
        report = {
            "report_id": report_id,
            "report_type": "comprehensive",
            "generated_at": datetime.now().isoformat(),
            "title": "Leviathan Exploitation Analysis Report",
            "summary": await self._generate_executive_summary(exploitation_data, safety_metrics),
            "exploitation_analysis": await self._analyze_exploitation_data(exploitation_data),
            "safety_assessment": await self._assess_safety_metrics(safety_metrics),
            "performance_metrics": await self._analyze_performance_data(performance_data),
            "recommendations": await self._generate_recommendations(exploitation_data, safety_metrics),
            "raw_data": {
                "exploitation": exploitation_data,
                "safety": safety_metrics,
                "performance": performance_data
            }
        }

        return report

    async def _generate_safety_report(self, report_id: str, safety_metrics: Dict[str, Any],
                                    config: Dict[str, Any]) -> Dict[str, Any]:
        """Generate a safety-focused report."""
        report = {
            "report_id": report_id,
            "report_type": "safety",
            "generated_at": datetime.now().isoformat(),
            "title": "Leviathan Safety Containment Report",
            "safety_assessment": await self._assess_safety_metrics(safety_metrics),
            "containment_effectiveness": await self._analyze_containment_effectiveness(safety_metrics),
            "risk_assessment": await self._assess_risks(safety_metrics),
            "safety_recommendations": await self._generate_safety_recommendations(safety_metrics)
        }

        return report

    async def _generate_performance_report(self, report_id: str, performance_data: Dict[str, Any],
                                         config: Dict[str, Any]) -> Dict[str, Any]:
        """Generate a performance-focused report."""
        report = {
            "report_id": report_id,
            "report_type": "performance",
            "generated_at": datetime.now().isoformat(),
            "title": "Leviathan Exploitation Performance Report",
            "performance_metrics": await self._analyze_performance_data(performance_data),
            "bottlenecks": await self._identify_bottlenecks(performance_data),
            "optimization_suggestions": await self._generate_optimization_suggestions(performance_data)
        }

        return report

    async def _generate_vulnerability_report(self, report_id: str, exploitation_data: Dict[str, Any],
                                           config: Dict[str, Any]) -> Dict[str, Any]:
        """Generate a vulnerability-focused report."""
        report = {
            "report_id": report_id,
            "report_type": "vulnerability",
            "generated_at": datetime.now().isoformat(),
            "title": "Leviathan Vulnerability Exploitation Report",
            "exploitation_analysis": await self._analyze_exploitation_data(exploitation_data),
            "vulnerability_trends": await self._analyze_vulnerability_trends(exploitation_data),
            "exploit_effectiveness": await self._assess_exploit_effectiveness(exploitation_data)
        }

        return report

    async def _generate_executive_summary(self, exploitation_data: Dict[str, Any],
                                        safety_metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Generate executive summary for comprehensive report."""
        total_exploits = exploitation_data.get("total_exploits", 0)
        successful_exploits = exploitation_data.get("successful_exploits", 0)
        safety_violations = safety_metrics.get("total_violations", 0)

        success_rate = (successful_exploits / total_exploits * 100) if total_exploits > 0 else 0

        risk_level = "Low"
        if safety_violations > 10:
            risk_level = "High"
        elif safety_violations > 5:
            risk_level = "Medium"

        return {
            "total_exploits_attempted": total_exploits,
            "successful_exploits": successful_exploits,
            "success_rate_percent": round(success_rate, 2),
            "safety_violations": safety_violations,
            "overall_risk_level": risk_level,
            "key_findings": [
                f"Exploit success rate: {success_rate:.1f}%",
                f"Safety violations detected: {safety_violations}",
                f"Risk assessment: {risk_level}"
            ]
        }

    async def _analyze_exploitation_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze exploitation data."""
        exploits = data.get("exploits", [])
        pipelines = data.get("pipelines", [])

        # Calculate statistics
        exploit_types = {}
        vulnerability_types = {}
        success_rates = []

        for exploit in exploits:
            exp_type = exploit.get("type", "unknown")
            vuln_type = exploit.get("vulnerability_type", "unknown")
            success = exploit.get("success", False)

            exploit_types[exp_type] = exploit_types.get(exp_type, 0) + 1
            vulnerability_types[vuln_type] = vulnerability_types.get(vuln_type, 0) + 1
            success_rates.append(1 if success else 0)

        avg_success_rate = statistics.mean(success_rates) if success_rates else 0

        # Pipeline analysis
        pipeline_success = sum(1 for p in pipelines if p.get("success", False))
        pipeline_success_rate = (pipeline_success / len(pipelines) * 100) if pipelines else 0

        return {
            "total_exploits": len(exploits),
            "exploit_types_distribution": exploit_types,
            "vulnerability_types_distribution": vulnerability_types,
            "average_success_rate": round(avg_success_rate * 100, 2),
            "pipeline_analysis": {
                "total_pipelines": len(pipelines),
                "successful_pipelines": pipeline_success,
                "success_rate_percent": round(pipeline_success_rate, 2)
            },
            "top_successful_exploits": sorted(
                [e for e in exploits if e.get("success")],
                key=lambda x: x.get("reliability_score", 0),
                reverse=True
            )[:5]
        }

    async def _assess_safety_metrics(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Assess safety metrics."""
        violations = metrics.get("violations", [])
        containment_events = metrics.get("containment_events", [])
        resource_usage = metrics.get("resource_usage", {})

        # Categorize violations
        violation_types = {}
        for violation in violations:
            v_type = violation.get("type", "unknown")
            violation_types[v_type] = violation_types.get(v_type, 0) + 1

        # Assess containment effectiveness
        containment_score = self._calculate_containment_score(containment_events)

        # Resource usage analysis
        resource_risks = self._analyze_resource_risks(resource_usage)

        return {
            "total_violations": len(violations),
            "violation_types": violation_types,
            "containment_effectiveness_score": containment_score,
            "resource_usage_risks": resource_risks,
            "safety_status": "good" if len(violations) == 0 else "warning" if len(violations) < 5 else "critical",
            "critical_violations": [v for v in violations if v.get("severity") == "critical"]
        }

    async def _analyze_performance_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze performance data."""
        execution_times = data.get("execution_times", [])
        resource_usage = data.get("resource_usage", {})
        throughput_metrics = data.get("throughput", {})

        # Calculate performance statistics
        if execution_times:
            avg_time = statistics.mean(execution_times)
            median_time = statistics.median(execution_times)
            p95_time = self._calculate_percentile(execution_times, 95)
            p99_time = self._calculate_percentile(execution_times, 99)
        else:
            avg_time = median_time = p95_time = p99_time = 0

        # Resource efficiency
        cpu_efficiency = self._calculate_cpu_efficiency(resource_usage)
        memory_efficiency = self._calculate_memory_efficiency(resource_usage)

        return {
            "execution_time_stats": {
                "average_seconds": round(avg_time, 2),
                "median_seconds": round(median_time, 2),
                "p95_seconds": round(p95_time, 2),
                "p99_seconds": round(p99_time, 2),
                "total_executions": len(execution_times)
            },
            "resource_efficiency": {
                "cpu_efficiency_score": cpu_efficiency,
                "memory_efficiency_score": memory_efficiency
            },
            "throughput_metrics": throughput_metrics,
            "performance_rating": self._calculate_performance_rating(avg_time, cpu_efficiency, memory_efficiency)
        }

    async def _analyze_containment_effectiveness(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze containment effectiveness."""
        events = metrics.get("containment_events", [])

        # Analyze different types of containment events
        prevention_events = [e for e in events if e.get("type") == "prevention"]
        mitigation_events = [e for e in events if e.get("type") == "mitigation"]
        failure_events = [e for e in events if e.get("type") == "failure"]

        effectiveness_score = (
            (len(prevention_events) * 1.0 + len(mitigation_events) * 0.5) /
            max(len(events), 1) * 100
        )

        return {
            "prevention_events": len(prevention_events),
            "mitigation_events": len(mitigation_events),
            "failure_events": len(failure_events),
            "effectiveness_score": round(effectiveness_score, 2),
            "containment_rating": self._rate_containment_effectiveness(effectiveness_score)
        }

    async def _assess_risks(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """Assess overall risks."""
        violations = metrics.get("violations", [])
        resource_usage = metrics.get("resource_usage", {})

        # Calculate risk scores
        violation_risk = min(len(violations) * 10, 100)  # Max 100 points
        resource_risk = self._calculate_resource_risk_score(resource_usage)

        total_risk_score = (violation_risk + resource_risk) / 2

        risk_level = "Low"
        if total_risk_score > 75:
            risk_level = "Critical"
        elif total_risk_score > 50:
            risk_level = "High"
        elif total_risk_score > 25:
            risk_level = "Medium"

        return {
            "violation_risk_score": violation_risk,
            "resource_risk_score": resource_risk,
            "total_risk_score": round(total_risk_score, 2),
            "risk_level": risk_level,
            "risk_factors": self._identify_risk_factors(violations, resource_usage)
        }

    async def _analyze_vulnerability_trends(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze vulnerability exploitation trends."""
        exploits = data.get("exploits", [])

        # Group by vulnerability type and time
        vuln_trends = {}
        for exploit in exploits:
            vuln_type = exploit.get("vulnerability_type", "unknown")
            timestamp = exploit.get("timestamp", datetime.now().isoformat())[:10]  # Date only

            if vuln_type not in vuln_trends:
                vuln_trends[vuln_type] = {}

            vuln_trends[vuln_type][timestamp] = vuln_trends[vuln_type].get(timestamp, 0) + 1

        # Calculate trend directions
        trends = {}
        for vuln_type, dates in vuln_trends.items():
            sorted_dates = sorted(dates.keys())
            if len(sorted_dates) >= 2:
                recent = sum(dates[d] for d in sorted_dates[-3:])  # Last 3 periods
                earlier = sum(dates[d] for d in sorted_dates[:-3]) if len(sorted_dates) > 3 else 0
                trend = "increasing" if recent > earlier else "decreasing" if recent < earlier else "stable"
                trends[vuln_type] = trend

        return {
            "vulnerability_trends": vuln_trends,
            "trend_analysis": trends,
            "most_targeted_vulnerabilities": sorted(
                vuln_trends.keys(),
                key=lambda x: sum(vuln_trends[x].values()),
                reverse=True
            )[:5]
        }

    async def _assess_exploit_effectiveness(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Assess exploit effectiveness."""
        exploits = data.get("exploits", [])

        # Calculate effectiveness metrics
        successful_exploits = [e for e in exploits if e.get("success")]
        reliability_scores = [e.get("reliability_score", 0) for e in successful_exploits]

        if reliability_scores:
            avg_reliability = statistics.mean(reliability_scores)
            max_reliability = max(reliability_scores)
        else:
            avg_reliability = max_reliability = 0

        # Effectiveness by exploit type
        effectiveness_by_type = {}
        for exploit in exploits:
            exp_type = exploit.get("type", "unknown")
            if exp_type not in effectiveness_by_type:
                effectiveness_by_type[exp_type] = {"total": 0, "successful": 0}

            effectiveness_by_type[exp_type]["total"] += 1
            if exploit.get("success"):
                effectiveness_by_type[exp_type]["successful"] += 1

        for exp_type in effectiveness_by_type:
            total = effectiveness_by_type[exp_type]["total"]
            successful = effectiveness_by_type[exp_type]["successful"]
            effectiveness_by_type[exp_type]["success_rate"] = (successful / total * 100) if total > 0 else 0

        return {
            "overall_success_rate": (len(successful_exploits) / len(exploits) * 100) if exploits else 0,
            "average_reliability_score": round(avg_reliability, 2),
            "highest_reliability_score": round(max_reliability, 2),
            "effectiveness_by_type": effectiveness_by_type,
            "most_effective_exploits": sorted(
                successful_exploits,
                key=lambda x: x.get("reliability_score", 0),
                reverse=True
            )[:3]
        }

    async def _generate_recommendations(self, exploitation_data: Dict[str, Any],
                                      safety_metrics: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on analysis."""
        recommendations = []

        # Based on exploitation success
        success_rate = exploitation_data.get("success_rate", 0)
        if success_rate < 50:
            recommendations.append("Improve exploit development techniques - current success rate is below 50%")
        elif success_rate > 90:
            recommendations.append("High success rate indicates robust exploitation capabilities")

        # Based on safety violations
        violations = safety_metrics.get("total_violations", 0)
        if violations > 10:
            recommendations.append("Critical: High number of safety violations detected - review containment procedures")
        elif violations > 0:
            recommendations.append("Address safety violations to improve containment effectiveness")

        # Based on performance
        avg_time = exploitation_data.get("average_execution_time", 0)
        if avg_time > 60:
            recommendations.append("Optimize execution performance - average time exceeds 1 minute")

        # General recommendations
        recommendations.extend([
            "Regularly update exploit database with latest vulnerabilities",
            "Implement automated testing for exploit reliability",
            "Monitor resource usage patterns for optimization opportunities",
            "Conduct periodic safety audits of containment mechanisms"
        ])

        return recommendations

    async def _generate_safety_recommendations(self, metrics: Dict[str, Any]) -> List[str]:
        """Generate safety-specific recommendations."""
        recommendations = []
        violations = metrics.get("violations", [])

        violation_types = {}
        for v in violations:
            v_type = v.get("type", "unknown")
            violation_types[v_type] = violation_types.get(v_type, 0) + 1

        # Recommendations based on violation types
        if violation_types.get("resource_limit_exceeded", 0) > 0:
            recommendations.append("Increase resource limits or optimize resource usage")

        if violation_types.get("network_connection_attempt", 0) > 0:
            recommendations.append("Strengthen network isolation in containment environments")

        if violation_types.get("unsafe_command", 0) > 0:
            recommendations.append("Implement stricter command validation and filtering")

        recommendations.extend([
            "Regular security audits of containment mechanisms",
            "Implement multi-layer safety controls",
            "Monitor and log all containment activities",
            "Develop incident response procedures for safety violations"
        ])

        return recommendations

    async def _generate_optimization_suggestions(self, data: Dict[str, Any]) -> List[str]:
        """Generate performance optimization suggestions."""
        suggestions = []

        execution_times = data.get("execution_times", [])
        if execution_times:
            avg_time = statistics.mean(execution_times)
            if avg_time > 30:
                suggestions.append("Consider parallelizing exploit execution for better performance")
            if avg_time > 10:
                suggestions.append("Optimize payload generation and encoding processes")

        resource_usage = data.get("resource_usage", {})
        cpu_usage = resource_usage.get("cpu_percent", 0)
        memory_usage = resource_usage.get("memory_mb", 0)

        if cpu_usage > 80:
            suggestions.append("High CPU usage detected - consider distributing workload")
        if memory_usage > 500:
            suggestions.append("High memory consumption - implement memory optimization techniques")

        suggestions.extend([
            "Cache frequently used payloads to reduce generation time",
            "Implement exploit result caching for repeated scenarios",
            "Use asynchronous processing for I/O bound operations",
            "Profile code performance to identify bottlenecks"
        ])

        return suggestions

    async def _identify_bottlenecks(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Identify performance bottlenecks."""
        bottlenecks = []

        execution_times = data.get("execution_times", [])
        if execution_times:
            p95_time = self._calculate_percentile(execution_times, 95)
            if p95_time > 60:
                bottlenecks.append({
                    "type": "execution_time",
                    "severity": "high",
                    "description": f"P95 execution time is {p95_time:.2f}s, indicating performance issues"
                })

        resource_usage = data.get("resource_usage", {})
        if resource_usage.get("cpu_percent", 0) > 90:
            bottlenecks.append({
                "type": "cpu_usage",
                "severity": "high",
                "description": "CPU usage consistently above 90%"
            })

        if resource_usage.get("memory_mb", 0) > 1000:
            bottlenecks.append({
                "type": "memory_usage",
                "severity": "high",
                "description": "Memory usage exceeds 1GB"
            })

        return bottlenecks

    def _calculate_containment_score(self, events: List[Dict[str, Any]]) -> float:
        """Calculate containment effectiveness score."""
        if not events:
            return 100.0  # Perfect score if no events

        prevention_score = sum(1 for e in events if e.get("type") == "prevention")
        mitigation_score = sum(0.5 for e in events if e.get("type") == "mitigation")
        failure_score = sum(-1 for e in events if e.get("type") == "failure")

        total_score = prevention_score + mitigation_score + failure_score
        max_possible = len(events) * 1.0  # Best case: all preventions

        return max(0, min(100, (total_score / max_possible) * 100)) if max_possible > 0 else 100

    def _analyze_resource_risks(self, usage: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze resource usage risks."""
        risks = {}

        cpu_percent = usage.get("cpu_percent", 0)
        if cpu_percent > 80:
            risks["cpu"] = {"level": "high", "usage": cpu_percent}
        elif cpu_percent > 50:
            risks["cpu"] = {"level": "medium", "usage": cpu_percent}

        memory_mb = usage.get("memory_mb", 0)
        if memory_mb > 512:
            risks["memory"] = {"level": "high", "usage": memory_mb}
        elif memory_mb > 256:
            risks["memory"] = {"level": "medium", "usage": memory_mb}

        return risks

    def _calculate_resource_risk_score(self, usage: Dict[str, Any]) -> float:
        """Calculate resource risk score."""
        cpu_risk = min(usage.get("cpu_percent", 0) / 100 * 50, 50)  # Max 50 points
        memory_risk = min(usage.get("memory_mb", 0) / 1024 * 50, 50)  # Max 50 points

        return cpu_risk + memory_risk

    def _identify_risk_factors(self, violations: List[Dict[str, Any]], usage: Dict[str, Any]) -> List[str]:
        """Identify risk factors."""
        factors = []

        if len(violations) > 5:
            factors.append("High number of safety violations")

        if usage.get("cpu_percent", 0) > 80:
            factors.append("Excessive CPU usage")

        if usage.get("memory_mb", 0) > 512:
            factors.append("High memory consumption")

        violation_types = set(v.get("type", "unknown") for v in violations)
        if "network_connection_attempt" in violation_types:
            factors.append("Network access attempts detected")

        if "unsafe_command" in violation_types:
            factors.append("Unsafe command execution attempts")

        return factors

    def _calculate_percentile(self, data: List[float], percentile: float) -> float:
        """Calculate percentile from data."""
        if not data:
            return 0
        data_sorted = sorted(data)
        index = (len(data_sorted) - 1) * (percentile / 100)
        lower = int(index)
        upper = lower + 1
        weight = index - lower

        if upper >= len(data_sorted):
            return data_sorted[lower]

        return data_sorted[lower] * (1 - weight) + data_sorted[upper] * weight

    def _calculate_cpu_efficiency(self, usage: Dict[str, Any]) -> float:
        """Calculate CPU efficiency score."""
        cpu_percent = usage.get("cpu_percent", 0)
        # Higher efficiency when CPU usage is moderate (not too low or high)
        if cpu_percent < 10:
            return 50  # Underutilized
        elif cpu_percent > 90:
            return 30  # Overutilized
        else:
            return 100 - abs(50 - cpu_percent) * 2  # Optimal around 50%

    def _calculate_memory_efficiency(self, usage: Dict[str, Any]) -> float:
        """Calculate memory efficiency score."""
        memory_mb = usage.get("memory_mb", 0)
        # Assume optimal memory usage is around 128-256MB
        if memory_mb < 64:
            return 60  # Possibly underutilized
        elif memory_mb > 512:
            return 20  # High usage
        else:
            return 100 - abs(192 - memory_mb) / 192 * 40  # Optimal around 192MB

    def _calculate_performance_rating(self, avg_time: float, cpu_eff: float, mem_eff: float) -> str:
        """Calculate overall performance rating."""
        time_score = max(0, 100 - avg_time / 10)  # Better with lower time
        efficiency_score = (cpu_eff + mem_eff) / 2
        overall_score = (time_score + efficiency_score) / 2

        if overall_score > 80:
            return "Excellent"
        elif overall_score > 60:
            return "Good"
        elif overall_score > 40:
            return "Fair"
        else:
            return "Poor"

    def _rate_containment_effectiveness(self, score: float) -> str:
        """Rate containment effectiveness."""
        if score > 90:
            return "Excellent"
        elif score > 75:
            return "Good"
        elif score > 60:
            return "Fair"
        else:
            return "Poor"

    async def _export_report(self, report: Dict[str, Any], export_path: str) -> None:
        """Export report to file."""
        try:
            path = Path(export_path)
            path.parent.mkdir(parents=True, exist_ok=True)

            if export_path.endswith('.json'):
                with open(path, 'w') as f:
                    json.dump(report, f, indent=2, default=str)
            else:
                # Plain text export
                with open(path, 'w') as f:
                    f.write(f"Leviathan Exploitation Report\n")
                    f.write(f"Generated: {report['generated_at']}\n")
                    f.write(f"Type: {report['report_type']}\n\n")

                    if 'summary' in report:
                        f.write("Executive Summary:\n")
                        for key, value in report['summary'].items():
                            f.write(f"  {key}: {value}\n")
                        f.write("\n")

                    # Add other sections as needed

            self.logger.info("Report exported", path=str(path))

        except Exception as e:
            self.logger.error("Report export failed", path=export_path, error=str(e))

    def get_report_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """Get report generation history."""
        return self.reports_history[-limit:]

    def get_report_by_id(self, report_id: str) -> Optional[Dict[str, Any]]:
        """Get a specific report by ID."""
        for report in self.reports_history:
            if report.get("report_id") == report_id:
                return report
        return None